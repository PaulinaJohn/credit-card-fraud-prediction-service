{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HXiwDdtY3miI"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","# from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import cross_val_score , StratifiedKFold\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","import warnings\n","warnings. filterwarnings('ignore')\n","import datetime as dt\n","import joblib\n","\n","# pd.pandas.set_option('display.max_columns', None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waR8qvQu3miM","outputId":"45af2bdf-5349-44a4-9d2d-bafdc1b17834"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>...</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>149.62</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>...</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>378.66</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>123.50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0</td>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>...</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>69.99</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 31 columns</p>\n","</div>"],"text/plain":["   Time        V1        V2        V3        V4        V5        V6        V7  \\\n","0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","\n","         V8        V9  ...       V21       V22       V23       V24       V25  \\\n","0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n","1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n","2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n","3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n","4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n","\n","        V26       V27       V28  Amount  Class  \n","0 -0.189115  0.133558 -0.021053  149.62      0  \n","1  0.125895 -0.008983  0.014724    2.69      0  \n","2 -0.139097 -0.055353 -0.059752  378.66      0  \n","3 -0.221929  0.062723  0.061458  123.50      0  \n","4  0.502292  0.219422  0.215153   69.99      0  \n","\n","[5 rows x 31 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["credit_card_data = pd.read_csv(\"creditcard.csv\")\n","\n","credit_card_data.head()"]},{"cell_type":"markdown","metadata":{"id":"t-DUKMF_3miN"},"source":["__Exploring the data and cleaning along__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5N7eZIAB3miO","outputId":"73158328-d4b4-485f-d1e4-0c9a36de83ef"},"outputs":[{"data":{"text/plain":["(284807, 31)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# checking the shape of the data\n","\n","credit_card_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yno5yAq_3miP","outputId":"4aad8314-9119-45d6-9ba8-c365922ec845"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 284807 entries, 0 to 284806\n","Data columns (total 31 columns):\n"," #   Column  Non-Null Count   Dtype  \n","---  ------  --------------   -----  \n"," 0   Time    284807 non-null  float64\n"," 1   V1      284807 non-null  float64\n"," 2   V2      284807 non-null  float64\n"," 3   V3      284807 non-null  float64\n"," 4   V4      284807 non-null  float64\n"," 5   V5      284807 non-null  float64\n"," 6   V6      284807 non-null  float64\n"," 7   V7      284807 non-null  float64\n"," 8   V8      284807 non-null  float64\n"," 9   V9      284807 non-null  float64\n"," 10  V10     284807 non-null  float64\n"," 11  V11     284807 non-null  float64\n"," 12  V12     284807 non-null  float64\n"," 13  V13     284807 non-null  float64\n"," 14  V14     284807 non-null  float64\n"," 15  V15     284807 non-null  float64\n"," 16  V16     284807 non-null  float64\n"," 17  V17     284807 non-null  float64\n"," 18  V18     284807 non-null  float64\n"," 19  V19     284807 non-null  float64\n"," 20  V20     284807 non-null  float64\n"," 21  V21     284807 non-null  float64\n"," 22  V22     284807 non-null  float64\n"," 23  V23     284807 non-null  float64\n"," 24  V24     284807 non-null  float64\n"," 25  V25     284807 non-null  float64\n"," 26  V26     284807 non-null  float64\n"," 27  V27     284807 non-null  float64\n"," 28  V28     284807 non-null  float64\n"," 29  Amount  284807 non-null  float64\n"," 30  Class   284807 non-null  int64  \n","dtypes: float64(30), int64(1)\n","memory usage: 67.4 MB\n"]}],"source":["credit_card_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhVWpHhl3miP","outputId":"aff9f737-da30-48b6-a9da-9bdd1015fb08"},"outputs":[{"data":{"text/plain":["Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n","       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n","       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n","       'Class'],\n","      dtype='object')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["credit_card_data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjBWNYKi3miQ","outputId":"6ffa24ae-5e91-423c-fab8-1d55f9ba3890"},"outputs":[{"data":{"text/plain":["Time      0\n","V1        0\n","V2        0\n","V3        0\n","V4        0\n","V5        0\n","V6        0\n","V7        0\n","V8        0\n","V9        0\n","V10       0\n","V11       0\n","V12       0\n","V13       0\n","V14       0\n","V15       0\n","V16       0\n","V17       0\n","V18       0\n","V19       0\n","V20       0\n","V21       0\n","V22       0\n","V23       0\n","V24       0\n","V25       0\n","V26       0\n","V27       0\n","V28       0\n","Amount    0\n","Class     0\n","dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# checking for missing values\n","\n","credit_card_data.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"XEWSgr4w3miQ"},"source":["No null values in the data. I also confirmed columns like 'V25', 'V26' which are hidden in the output above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1YDgRKl3miQ","outputId":"5c25145e-b113-45bd-d5c0-b2fcd33c1639"},"outputs":[{"data":{"text/plain":["1081"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# checking for duplicates\n","\n","credit_card_data.duplicated().sum()"]},{"cell_type":"markdown","metadata":{"id":"MYC9QVzp3miR"},"source":["There are duplicate records in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3Djd4x-3miR","outputId":"d3a3ba06-0bdb-412d-cc87-9eaee8eeadf0"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# let's drop duplicate records\n","\n","credit_card_data.drop_duplicates(inplace=True)\n","\n","# check for duplicates again\n","credit_card_data.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC_bDqaG3miR","outputId":"3a80b7d9-84ce-4ad9-efd5-28e66f338bdc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>...</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","      <td>283726.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>94811.077600</td>\n","      <td>0.005917</td>\n","      <td>-0.004135</td>\n","      <td>0.001613</td>\n","      <td>-0.002966</td>\n","      <td>0.001828</td>\n","      <td>-0.001139</td>\n","      <td>0.001801</td>\n","      <td>-0.000854</td>\n","      <td>-0.001596</td>\n","      <td>...</td>\n","      <td>-0.000371</td>\n","      <td>-0.000015</td>\n","      <td>0.000198</td>\n","      <td>0.000214</td>\n","      <td>-0.000232</td>\n","      <td>0.000149</td>\n","      <td>0.001763</td>\n","      <td>0.000547</td>\n","      <td>88.472687</td>\n","      <td>0.001667</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>47481.047891</td>\n","      <td>1.948026</td>\n","      <td>1.646703</td>\n","      <td>1.508682</td>\n","      <td>1.414184</td>\n","      <td>1.377008</td>\n","      <td>1.331931</td>\n","      <td>1.227664</td>\n","      <td>1.179054</td>\n","      <td>1.095492</td>\n","      <td>...</td>\n","      <td>0.723909</td>\n","      <td>0.724550</td>\n","      <td>0.623702</td>\n","      <td>0.605627</td>\n","      <td>0.521220</td>\n","      <td>0.482053</td>\n","      <td>0.395744</td>\n","      <td>0.328027</td>\n","      <td>250.399437</td>\n","      <td>0.040796</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>-56.407510</td>\n","      <td>-72.715728</td>\n","      <td>-48.325589</td>\n","      <td>-5.683171</td>\n","      <td>-113.743307</td>\n","      <td>-26.160506</td>\n","      <td>-43.557242</td>\n","      <td>-73.216718</td>\n","      <td>-13.434066</td>\n","      <td>...</td>\n","      <td>-34.830382</td>\n","      <td>-10.933144</td>\n","      <td>-44.807735</td>\n","      <td>-2.836627</td>\n","      <td>-10.295397</td>\n","      <td>-2.604551</td>\n","      <td>-22.565679</td>\n","      <td>-15.430084</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>54204.750000</td>\n","      <td>-0.915951</td>\n","      <td>-0.600321</td>\n","      <td>-0.889682</td>\n","      <td>-0.850134</td>\n","      <td>-0.689830</td>\n","      <td>-0.769031</td>\n","      <td>-0.552509</td>\n","      <td>-0.208828</td>\n","      <td>-0.644221</td>\n","      <td>...</td>\n","      <td>-0.228305</td>\n","      <td>-0.542700</td>\n","      <td>-0.161703</td>\n","      <td>-0.354453</td>\n","      <td>-0.317485</td>\n","      <td>-0.326763</td>\n","      <td>-0.070641</td>\n","      <td>-0.052818</td>\n","      <td>5.600000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>84692.500000</td>\n","      <td>0.020384</td>\n","      <td>0.063949</td>\n","      <td>0.179963</td>\n","      <td>-0.022248</td>\n","      <td>-0.053468</td>\n","      <td>-0.275168</td>\n","      <td>0.040859</td>\n","      <td>0.021898</td>\n","      <td>-0.052596</td>\n","      <td>...</td>\n","      <td>-0.029441</td>\n","      <td>0.006675</td>\n","      <td>-0.011159</td>\n","      <td>0.041016</td>\n","      <td>0.016278</td>\n","      <td>-0.052172</td>\n","      <td>0.001479</td>\n","      <td>0.011288</td>\n","      <td>22.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>139298.000000</td>\n","      <td>1.316068</td>\n","      <td>0.800283</td>\n","      <td>1.026960</td>\n","      <td>0.739647</td>\n","      <td>0.612218</td>\n","      <td>0.396792</td>\n","      <td>0.570474</td>\n","      <td>0.325704</td>\n","      <td>0.595977</td>\n","      <td>...</td>\n","      <td>0.186194</td>\n","      <td>0.528245</td>\n","      <td>0.147748</td>\n","      <td>0.439738</td>\n","      <td>0.350667</td>\n","      <td>0.240261</td>\n","      <td>0.091208</td>\n","      <td>0.078276</td>\n","      <td>77.510000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>172792.000000</td>\n","      <td>2.454930</td>\n","      <td>22.057729</td>\n","      <td>9.382558</td>\n","      <td>16.875344</td>\n","      <td>34.801666</td>\n","      <td>73.301626</td>\n","      <td>120.589494</td>\n","      <td>20.007208</td>\n","      <td>15.594995</td>\n","      <td>...</td>\n","      <td>27.202839</td>\n","      <td>10.503090</td>\n","      <td>22.528412</td>\n","      <td>4.584549</td>\n","      <td>7.519589</td>\n","      <td>3.517346</td>\n","      <td>31.612198</td>\n","      <td>33.847808</td>\n","      <td>25691.160000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows Ã— 31 columns</p>\n","</div>"],"text/plain":["                Time             V1             V2             V3  \\\n","count  283726.000000  283726.000000  283726.000000  283726.000000   \n","mean    94811.077600       0.005917      -0.004135       0.001613   \n","std     47481.047891       1.948026       1.646703       1.508682   \n","min         0.000000     -56.407510     -72.715728     -48.325589   \n","25%     54204.750000      -0.915951      -0.600321      -0.889682   \n","50%     84692.500000       0.020384       0.063949       0.179963   \n","75%    139298.000000       1.316068       0.800283       1.026960   \n","max    172792.000000       2.454930      22.057729       9.382558   \n","\n","                  V4             V5             V6             V7  \\\n","count  283726.000000  283726.000000  283726.000000  283726.000000   \n","mean       -0.002966       0.001828      -0.001139       0.001801   \n","std         1.414184       1.377008       1.331931       1.227664   \n","min        -5.683171    -113.743307     -26.160506     -43.557242   \n","25%        -0.850134      -0.689830      -0.769031      -0.552509   \n","50%        -0.022248      -0.053468      -0.275168       0.040859   \n","75%         0.739647       0.612218       0.396792       0.570474   \n","max        16.875344      34.801666      73.301626     120.589494   \n","\n","                  V8             V9  ...            V21            V22  \\\n","count  283726.000000  283726.000000  ...  283726.000000  283726.000000   \n","mean       -0.000854      -0.001596  ...      -0.000371      -0.000015   \n","std         1.179054       1.095492  ...       0.723909       0.724550   \n","min       -73.216718     -13.434066  ...     -34.830382     -10.933144   \n","25%        -0.208828      -0.644221  ...      -0.228305      -0.542700   \n","50%         0.021898      -0.052596  ...      -0.029441       0.006675   \n","75%         0.325704       0.595977  ...       0.186194       0.528245   \n","max        20.007208      15.594995  ...      27.202839      10.503090   \n","\n","                 V23            V24            V25            V26  \\\n","count  283726.000000  283726.000000  283726.000000  283726.000000   \n","mean        0.000198       0.000214      -0.000232       0.000149   \n","std         0.623702       0.605627       0.521220       0.482053   \n","min       -44.807735      -2.836627     -10.295397      -2.604551   \n","25%        -0.161703      -0.354453      -0.317485      -0.326763   \n","50%        -0.011159       0.041016       0.016278      -0.052172   \n","75%         0.147748       0.439738       0.350667       0.240261   \n","max        22.528412       4.584549       7.519589       3.517346   \n","\n","                 V27            V28         Amount          Class  \n","count  283726.000000  283726.000000  283726.000000  283726.000000  \n","mean        0.001763       0.000547      88.472687       0.001667  \n","std         0.395744       0.328027     250.399437       0.040796  \n","min       -22.565679     -15.430084       0.000000       0.000000  \n","25%        -0.070641      -0.052818       5.600000       0.000000  \n","50%         0.001479       0.011288      22.000000       0.000000  \n","75%         0.091208       0.078276      77.510000       0.000000  \n","max        31.612198      33.847808   25691.160000       1.000000  \n","\n","[8 rows x 31 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# let's see summary statistics\n","\n","credit_card_data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14PzR1G53miS","outputId":"b7d807e6-6a8c-4045-a234-67dfcb0dbe4a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number of records</th>\n","    </tr>\n","    <tr>\n","      <th>Class</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>283253</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>473</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       number of records\n","Class                   \n","0                 283253\n","1                    473"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#Checking how the classes are represented\n","\n","credit_card_data.groupby(\"Class\")[[\"Class\"]].count().rename({\"Class\": \"number of records\"}, axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"G0dQLWAo3miS"},"source":["We see that there's class imbalance. However, this is typical of such scenarios in real life, in which there are usually more non-suspicious than suspicious transaction activities. If we cannot get more data for the minority class, we leave the data as-is and not try to address the imbalance issue."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eF2KMOR23miS","outputId":"5952af7e-1b78-4675-f6b9-662f6352003a"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# more exploration; let's look at the amount by class\n","\n","sns.set_theme(rc={'figure.figsize':(8,6)})\n","sns.barplot(data = credit_card_data.groupby(\"Class\")[\"Amount\"].mean().reset_index(), x = \"Class\", y = \"Amount\")\n","plt.title(\"Average Amount by Non-fraudulent and Fraudulent Classes\", fontsize = 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FYZ16w7T3miS"},"source":["We can see already that, on average, the amount transacted in fraudulent instances are more than that transacted on normal or non-fraudulent instances. This could mean that higher transaction amounts could be an indication that a transaction is fraudulent- i.e, fraudulent transactions are characterized by higher amounts."]},{"cell_type":"markdown","metadata":{"id":"MN0bQ4Wm3miS"},"source":["We really do not know what v1 to v28 mean, as, because this is sensitive data, they were encoded. so, exploring the data further is a bit challenging.\n","\n","Now, let's move on to checking if there are any additional features we can create from existing features or, if there is even a need for this."]},{"cell_type":"markdown","metadata":{"id":"4m-GBPo43miS"},"source":["__Feature Engineering__"]},{"cell_type":"markdown","metadata":{"id":"sHoErRYQ3miS"},"source":["There are no feature engineering needs observed."]},{"cell_type":"markdown","metadata":{"id":"kbws1d143miS"},"source":["__Model development__\n","\n","All features and the target are already in numeric form so, no need for encoding. So, moving on to splitting the data and training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANiUCetA3miT"},"outputs":[],"source":["# splitting into training and testing data.\n","\n","train_dataset = credit_card_data.sample(frac=0.8, random_state=0)\n","test_dataset = credit_card_data.drop(train_dataset.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLVQKWsr3miT"},"outputs":[],"source":["# separating into feature and target\n","\n","X_train, y_train = train_dataset.drop(\"Class\", axis=1), train_dataset[\"Class\"]\n","X_test, y_test = test_dataset.drop(\"Class\", axis=1), test_dataset[\"Class\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IGN7TaJ3miT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_uGx0G53miT"},"outputs":[],"source":["# [17 14 12 10 16 11  9  7 18  4]\n","\n","# {\n","#   \"imput1\": 2.288644,\n","#   \"imput2\": 0.325574,\n","#   \"imput3\": -0.270953,\n","#   \"imput4\": -0.838587,\n","#   \"imput5\": -0.414575,\n","#   \"imput6\": -0.503141,\n","#   \"imput7\": -1.692029,\n","#   \"imput8\": 0.66678,\n","#   \"imput9\": 0.599717,\n","#   \"imput10\": 1.725321\n","# }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjcX_QeJ3miT"},"outputs":[],"source":["# Will first train a baseline model- using the usual train_test_split cross-validation.\n","# let's try a number of alorigthms\n","\n","# Instantiating the algorithms\n","\n","lg_model = LogisticRegression(multi_class = \"multinomial\", random_state = 42)\n","ds_model = DecisionTreeClassifier(random_state = 42)\n","rf_model = RandomForestClassifier(random_state= 42)\n","sv_model = SVC(random_state = 42)\n","lsv_model = LinearSVC(dual = False, random_state=42)\n","xg_model = XGBClassifier(random_state = 42)\n","# cb_model = clf = CatBoostClassifier(iterations=5, learning_rate=0.1)\n","ad_model = AdaBoostClassifier(random_state = 42)\n","lgm_model = LGBMClassifier(random_state = 42)\n","kn_model = KNeighborsClassifier()\n","nb_model = GaussianNB()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hISf50Yl3miT"},"outputs":[],"source":["models = {\"Logistic_regression\": lg_model,\n","          \"Decision_trees\": ds_model,\n","          \"Random_forest\": rf_model,\n","          \"Support_vector\": sv_model,\n","          \"Linear_support_vector\": lsv_model,\n","          \"Xgboost\": xg_model,\n","          \"Adaboost\": ad_model,\n","          \"LGBM\": lgm_model,\n","          \"KNN\": kn_model,\n","          \"Naive_bayes\": nb_model\n","          }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TIEfUwC3miT"},"outputs":[],"source":["# Will be using a pipeline so, instantiating the scaler too so I can add this to the pipeline-\n","# MinMax scaler because I am also trying out algorithms that use the gradiant descent- e.g., Logistic regression.\n","# Also, Algorithms like random forest don't care whether you scale your data or not so, for them, it doesn't matter what scaler you use.\n","\n","scaler = MinMaxScaler()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lnsKeP43miT","outputId":"9b3787a7-9208-42cd-8790-8289d11af230"},"outputs":[{"name":"stdout","output_type":"stream","text":["evaluation started at 2023-09-29 14:18:37.552942\n","The training score for Logistic_regression is 0.9991453029108163, \n","\n","The testing score for Logistic_regression is 0.9990483743061063 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.87      0.60      0.71       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.93      0.80      0.86     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Logistic_regression \n"," col_0      0   1\n","Class           \n","0      56624  10\n","1         44  67 \n","\n","------------------------ \n","\n","The training score for Decision_trees is 1.0, \n","\n","The testing score for Decision_trees is 0.9991717331923518 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.78      0.80      0.79       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.89      0.90      0.90     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Decision_trees \n"," col_0      0   1\n","Class           \n","0      56609  25\n","1         22  89 \n","\n","------------------------ \n","\n","The training score for Random_forest is 1.0, \n","\n","The testing score for Random_forest is 0.9995418098510882 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.95      0.81      0.87       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.97      0.91      0.94     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Random_forest \n"," col_0      0   1\n","Class           \n","0      56629   5\n","1         21  90 \n","\n","------------------------ \n","\n","The training score for Lupport_vector is 0.999387613941255, \n","\n","The testing score for Lupport_vector is 0.9993303374746674 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.83      0.82      0.83       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.92      0.91      0.91     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Lupport_vector \n"," col_0      0   1\n","Class           \n","0      56616  18\n","1         20  91 \n","\n","------------------------ \n","\n","The training score for linear_support_vector is 0.9991849538067062, \n","\n","The testing score for linear_support_vector is 0.9990483743061063 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.85      0.62      0.72       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.93      0.81      0.86     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","linear_support_vector \n"," col_0      0   1\n","Class           \n","0      56622  12\n","1         42  69 \n","\n","------------------------ \n","\n","The training score for Xgboost is 1.0, \n","\n","The testing score for Xgboost is 0.9995418098510882 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.96      0.80      0.87       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.98      0.90      0.94     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Xgboost \n"," col_0      0   1\n","Class           \n","0      56630   4\n","1         22  89 \n","\n","------------------------ \n","\n","The training score for Adaboost is 0.9993303404249695, \n","\n","The testing score for Adaboost is 0.9992598466825271 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.85      0.76      0.80       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.92      0.88      0.90     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Adaboost \n"," col_0      0   1\n","Class           \n","0      56619  15\n","1         27  84 \n","\n","------------------------ \n","\n","[LightGBM] [Info] Number of positive: 362, number of negative: 226619\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020620 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 7650\n","[LightGBM] [Info] Number of data points in the train set: 226981, number of used features: 30\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001595 -> initscore=-6.439381\n","[LightGBM] [Info] Start training from score -6.439381\n","The training score for LGBM is 0.9794960811697896, \n","\n","The testing score for LGBM is 0.9782712133227597 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.98      0.99     56634\n","           1       0.05      0.59      0.10       111\n","\n","    accuracy                           0.98     56745\n","   macro avg       0.53      0.78      0.54     56745\n","weighted avg       1.00      0.98      0.99     56745\n"," \n","\n","LGBM \n"," col_0      0     1\n","Class             \n","0      55447  1187\n","1         46    65 \n","\n","------------------------ \n","\n","The training score for KNN is 0.9995638401452104, \n","\n","The testing score for KNN is 0.9994536963609129 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.93      0.78      0.85       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.96      0.89      0.92     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","KNN \n"," col_0      0   1\n","Class           \n","0      56627   7\n","1         24  87 \n","\n","------------------------ \n","\n","The training score for Naive_bayes is 0.9775796212017746, \n","\n","The testing score for Naive_bayes is 0.9783064587188298 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.98      0.99     56634\n","           1       0.07      0.87      0.14       111\n","\n","    accuracy                           0.98     56745\n","   macro avg       0.54      0.93      0.56     56745\n","weighted avg       1.00      0.98      0.99     56745\n"," \n","\n","Naive_bayes \n"," col_0      0     1\n","Class             \n","0      55417  1217\n","1         14    97 \n","\n","------------------------ \n","\n","evaluation ended at 2023-09-29 14:25:47.931592\n"]}],"source":["# function to evaluate the models from the algorithms\n","\n","# check precision, recall and f1_score\n","\n","print(f\"evaluation started at {dt.datetime.now()}\")\n","for name, model in models.items():\n","    pipeline = Pipeline([\n","        ('scaler', scaler),\n","        ('model', model)\n","        ])\n","    pipeline.fit(X_train, y_train)\n","    train_score = pipeline.score(X_train, y_train)\n","    print(f\"The training score for {name} is {train_score}, \\n\")\n","    test_score = pipeline.score(X_test, y_test)\n","    print(f\"The testing score for {name} is {test_score}\", \"\\n\")\n","    y_pred = pipeline.predict(X_test)\n","    print(\"classification_report: \", \"\\n\", classification_report(y_test, y_pred), '\\n')\n","    confusion_matrix = pd.crosstab(y_test, y_pred)\n","    print(name, \"\\n\", confusion_matrix, \"\\n\")\n","    print(\"------------------------\", \"\\n\")\n","print(f\"evaluation ended at {dt.datetime.now()}\")"]},{"cell_type":"markdown","metadata":{"id":"mIyVYw9A3miU"},"source":["- A good number of the models are doing well.\n","- The Naive_bayes model would have been the best in that it has the least mis-classification for the minority class However, It has a very poor precision (0.07) for the same class. It also mis-classifies quite a good number of the majority class (Although this may not be too significant if we consider the size of data for that class.)\n","- There are three models doing the best- The Support vector, Random forest and Xgboost models.\n","- For this baseline experiment, my decision on the best performing model will be based on what favours the minority class due to the high class imbalance so, I would go for the Support vector model as, of these three, it has the least mis-classification for the minority class and, it also has good precision, recall and F-1 score for this class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrhNQi7R3miU","outputId":"66488cb9-77a4-4ea8-da56-83a15c8f9040"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.999387613941255\n","0.9993303374746674\n"]},{"data":{"text/plain":["['model.joblib']"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# training only the support vector model and packaging it for deployment\n","\n","pipeline = Pipeline([\n","            ('scaler', scaler),\n","            ('model', sv_model)\n","            ])\n","\n","pipeline.fit(X_train, y_train)\n","print(pipeline.score(X_train, y_train))\n","print(pipeline.score(X_test, y_test))\n","\n","# joblib.dump(pipeline, 'model/model.joblib')"]},{"cell_type":"markdown","metadata":{"id":"7SJMgkRe3miU"},"source":["As already mentioned, the models did not do badly. However, let's see if they will still do as well if we reduce the number of features.\n","If so, then there's really no use using these number of features as that would mean the user have to supply all these variables as imput to the prediction engine.\n","\n","So, on to some feature selection.\n","\n","There are a number of approaches to this. Tree-based algorithms like Random forest and Xgboost aid feature selection as they provide feature importance scores. Then, there's also Chi-square, except that chi-square is better for categorical-predictor-categorical-target situations and, 29 of the 30 preditor variables in our dataset hold contnous data.\n","\n","So, we opt for the tree-based algorithm. Let's try random forest.\n","\n","After getting the most important variables, we subset these variables from the original data and only use these to retrain each of the models we were initially experimenting on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yum5UCrI3miU","outputId":"50f31600-1f16-4394-8b7d-660b2c78d1cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["The top 10 important features are: [17 14 12 10 16 11  9  7 18  4]\n"]}],"source":["# Instantiate the model\n","selection_model = RandomForestClassifier(random_state=42)\n","\n","# Train the model on your training data\n","selection_model.fit(X_train, y_train)\n","\n","feature_importances = selection_model.feature_importances_\n","\n","# Sort features by importance\n","sorted_feature_indices = np.argsort(feature_importances)[::-1]\n","\n","# Select the top N features\n","top_n_features = sorted_feature_indices[:10]\n","\n","print(\"The top 10 important features are:\", top_n_features)\n","\n","# X_train_selected = X_train[:, top_n_features]\n","# X_test_selected = X_test[:, top_n_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFwn2RAP3miU"},"outputs":[],"source":["# So now, subsetting credit_card_data for these features\n","\n","X_train_selected = X_train.iloc[:, top_n_features]\n","X_test_selected = X_test.iloc[:, top_n_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMJIf-0G3miU","outputId":"3f1a8c73-57d8-4aed-cd5f-0edd9f3c87fe"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>V17</th>\n","      <th>V14</th>\n","      <th>V12</th>\n","      <th>V10</th>\n","      <th>V16</th>\n","      <th>V11</th>\n","      <th>V9</th>\n","      <th>V7</th>\n","      <th>V18</th>\n","      <th>V4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","      <td>226981.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.002006</td>\n","      <td>0.000216</td>\n","      <td>-0.000541</td>\n","      <td>-0.000214</td>\n","      <td>0.002397</td>\n","      <td>0.000707</td>\n","      <td>-0.000226</td>\n","      <td>0.002673</td>\n","      <td>0.002476</td>\n","      <td>-0.003048</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.833136</td>\n","      <td>0.948755</td>\n","      <td>0.990213</td>\n","      <td>1.073013</td>\n","      <td>0.870480</td>\n","      <td>1.017941</td>\n","      <td>1.096637</td>\n","      <td>1.218908</td>\n","      <td>0.836368</td>\n","      <td>1.413769</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-25.162799</td>\n","      <td>-19.214325</td>\n","      <td>-18.683715</td>\n","      <td>-22.187089</td>\n","      <td>-14.129855</td>\n","      <td>-4.797473</td>\n","      <td>-10.842526</td>\n","      <td>-33.239328</td>\n","      <td>-9.498746</td>\n","      <td>-5.683171</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>-0.483526</td>\n","      <td>-0.425764</td>\n","      <td>-0.406231</td>\n","      <td>-0.536198</td>\n","      <td>-0.466849</td>\n","      <td>-0.760992</td>\n","      <td>-0.643990</td>\n","      <td>-0.553384</td>\n","      <td>-0.496722</td>\n","      <td>-0.850717</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>-0.066390</td>\n","      <td>0.049576</td>\n","      <td>0.139069</td>\n","      <td>-0.093162</td>\n","      <td>0.068010</td>\n","      <td>-0.032703</td>\n","      <td>-0.052480</td>\n","      <td>0.041081</td>\n","      <td>-0.000890</td>\n","      <td>-0.021210</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.399834</td>\n","      <td>0.492152</td>\n","      <td>0.616507</td>\n","      <td>0.453454</td>\n","      <td>0.523525</td>\n","      <td>0.740332</td>\n","      <td>0.597417</td>\n","      <td>0.570052</td>\n","      <td>0.503248</td>\n","      <td>0.740138</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>9.253526</td>\n","      <td>10.526766</td>\n","      <td>7.848392</td>\n","      <td>23.745136</td>\n","      <td>17.315112</td>\n","      <td>12.018913</td>\n","      <td>15.594995</td>\n","      <td>120.589494</td>\n","      <td>5.041069</td>\n","      <td>16.875344</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 V17            V14            V12            V10  \\\n","count  226981.000000  226981.000000  226981.000000  226981.000000   \n","mean        0.002006       0.000216      -0.000541      -0.000214   \n","std         0.833136       0.948755       0.990213       1.073013   \n","min       -25.162799     -19.214325     -18.683715     -22.187089   \n","25%        -0.483526      -0.425764      -0.406231      -0.536198   \n","50%        -0.066390       0.049576       0.139069      -0.093162   \n","75%         0.399834       0.492152       0.616507       0.453454   \n","max         9.253526      10.526766       7.848392      23.745136   \n","\n","                 V16            V11             V9             V7  \\\n","count  226981.000000  226981.000000  226981.000000  226981.000000   \n","mean        0.002397       0.000707      -0.000226       0.002673   \n","std         0.870480       1.017941       1.096637       1.218908   \n","min       -14.129855      -4.797473     -10.842526     -33.239328   \n","25%        -0.466849      -0.760992      -0.643990      -0.553384   \n","50%         0.068010      -0.032703      -0.052480       0.041081   \n","75%         0.523525       0.740332       0.597417       0.570052   \n","max        17.315112      12.018913      15.594995     120.589494   \n","\n","                 V18             V4  \n","count  226981.000000  226981.000000  \n","mean        0.002476      -0.003048  \n","std         0.836368       1.413769  \n","min        -9.498746      -5.683171  \n","25%        -0.496722      -0.850717  \n","50%        -0.000890      -0.021210  \n","75%         0.503248       0.740138  \n","max         5.041069      16.875344  "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["X_train_selected.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-m9kkPl3miV"},"outputs":[],"source":["# V17: -25.162799 to 9.253526\n","# V14: -19.214325 to 10.526766\n","# V12: -18.683715 to 7.848392\n","# V10: -22.187089 to 23.745136\n","# V16: -14.129855 to 17.315112\n","# V11: -4.797473\tto 12.018913\n","# V9: -10.842526\tto 15.594995\n","# V7: -33.239328\tto 120.589494\n","# V18: -9.498746 to 5.041069\n","# V4: -5.683171 to 16.875344\n","\n","# imput1: -25.162799 to 9.253526\n","# imput2: -19.214325 to 10.526766\n","# imput3: -18.683715 to 7.848392\n","# imput4: -22.187089 to 23.745136\n","# imput5: -14.129855 to 17.315112\n","# imput6: -4.797473 to 12.018913\n","# imput7: -10.842526 to 15.594995\n","# imput8: -33.239328 to 120.589494\n","# imput9: -9.498746 to 5.041069\n","# imput10: -5.683171 to 16.875344"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qOamcOn3miV","outputId":"2b57a574-9489-4a4e-b987-7c0935278410"},"outputs":[{"name":"stdout","output_type":"stream","text":["evaluation started at 2023-10-01 19:24:27.130746\n","The training score for Logistic_regression is 0.9991144633251241, \n","\n","The testing score for Logistic_regression is 0.998977883513966 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.85      0.58      0.69       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.93      0.79      0.84     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Logistic_regression \n"," col_0      0   1\n","Class           \n","0      56623  11\n","1         47  64 \n","\n","------------------------ \n","\n","The training score for Decision_trees is 1.0, \n","\n","The testing score for Decision_trees is 0.9991188650982465 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.77      0.78      0.78       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.88      0.89      0.89     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Decision_trees \n"," col_0      0   1\n","Class           \n","0      56608  26\n","1         24  87 \n","\n","------------------------ \n","\n","The training score for Random_forest is 1.0, \n","\n","The testing score for Random_forest is 0.9995594325491233 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.94      0.83      0.88       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.97      0.91      0.94     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Random_forest \n"," col_0      0   1\n","Class           \n","0      56628   6\n","1         19  92 \n","\n","------------------------ \n","\n","The training score for Support_vector is 0.9994008309065516, \n","\n","The testing score for Support_vector is 0.9993303374746674 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.84      0.81      0.83       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.92      0.91      0.91     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Support_vector \n"," col_0      0   1\n","Class           \n","0      56617  17\n","1         21  90 \n","\n","------------------------ \n","\n","The training score for Linear_support_vector is 0.9991232746353219, \n","\n","The testing score for Linear_support_vector is 0.9989955062120011 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.84      0.60      0.70       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.92      0.80      0.85     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Linear_support_vector \n"," col_0      0   1\n","Class           \n","0      56621  13\n","1         44  67 \n","\n","------------------------ \n","\n","The training score for Xgboost is 0.9999030755878245, \n","\n","The testing score for Xgboost is 0.9994184509648427 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.91      0.78      0.84       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.95      0.89      0.92     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Xgboost \n"," col_0      0   1\n","Class           \n","0      56625   9\n","1         24  87 \n","\n","------------------------ \n","\n","The training score for Adaboost is 0.9992466329780907, \n","\n","The testing score for Adaboost is 0.9991364877962816 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.82      0.72      0.77       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.91      0.86      0.88     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","Adaboost \n"," col_0      0   1\n","Class           \n","0      56616  18\n","1         31  80 \n","\n","------------------------ \n","\n","[LightGBM] [Info] Number of positive: 362, number of negative: 226619\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008232 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 226981, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001595 -> initscore=-6.439381\n","[LightGBM] [Info] Start training from score -6.439381\n","The training score for LGBM is 0.9983434736828193, \n","\n","The testing score for LGBM is 0.9968102916556525 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.35      0.72      0.47       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.67      0.86      0.73     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","LGBM \n"," col_0      0    1\n","Class            \n","0      56484  150\n","1         31   80 \n","\n","------------------------ \n","\n","The training score for KNN is 0.9995902740758037, \n","\n","The testing score for KNN is 0.9995065644550181 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56634\n","           1       0.94      0.80      0.86       111\n","\n","    accuracy                           1.00     56745\n","   macro avg       0.97      0.90      0.93     56745\n","weighted avg       1.00      1.00      1.00     56745\n"," \n","\n","KNN \n"," col_0      0   1\n","Class           \n","0      56628   6\n","1         22  89 \n","\n","------------------------ \n","\n","The training score for Naive_bayes is 0.9910697371145603, \n","\n","The testing score for Naive_bayes is 0.9915058595470967 \n","\n","classification_report:  \n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.99      1.00     56634\n","           1       0.17      0.87      0.29       111\n","\n","    accuracy                           0.99     56745\n","   macro avg       0.59      0.93      0.64     56745\n","weighted avg       1.00      0.99      0.99     56745\n"," \n","\n","Naive_bayes \n"," col_0      0    1\n","Class            \n","0      56166  468\n","1         14   97 \n","\n","------------------------ \n","\n","evaluation ended at 2023-10-01 19:27:55.475703\n"]}],"source":["# Now, re-evaluating our models with the new data\n","\n","print(f\"evaluation started at {dt.datetime.now()}\")\n","for name, model in models.items():\n","    pipeline = Pipeline([\n","        ('scaler', scaler),\n","        ('model', model)\n","        ])\n","    pipeline.fit(X_train_selected, y_train)\n","    train_score = pipeline.score(X_train_selected, y_train)\n","    print(f\"The training score for {name} is {train_score}, \\n\")\n","    test_score = pipeline.score(X_test_selected, y_test)\n","    print(f\"The testing score for {name} is {test_score}\", \"\\n\")\n","    y_pred_selected = pipeline.predict(X_test_selected)\n","    print(\"classification_report: \", \"\\n\", classification_report(y_test, y_pred_selected), '\\n')\n","    confusion_matrix = pd.crosstab(y_test, y_pred_selected)\n","    print(name, \"\\n\", confusion_matrix, \"\\n\")\n","    print(\"------------------------\", \"\\n\")\n","print(f\"evaluation ended at {dt.datetime.now()}\")"]},{"cell_type":"markdown","metadata":{"id":"GVSFd8Ce3miV"},"source":["- Some of the models got slightly better, a few slightly worse.\n","- KNN displaces the Xgboost model in thrid position as one of the best 3. However, Random forest edges the support vector as the best of the three this time- It least misclassified the minority class and its class-wise metrics are also very good.\n","However, on a side note, this good result could actually be because the algorithm used for feature selection- random forest- was also used to train this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8EtU6SF3miV","outputId":"db7252f1-8dd5-46d6-c8e7-5f0d7aa90c37"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.9995594325491233\n"]},{"data":{"text/plain":["['../model/model_v1.joblib']"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# Let's try our the random forest model in our service- writing to file.\n","\n","pipeline = Pipeline([\n","            ('scaler', scaler),\n","            ('model', rf_model)\n","            ])\n","\n","pipeline.fit(X_train_selected, y_train)\n","print(pipeline.score(X_train_selected, y_train))\n","print(pipeline.score(X_test_selected, y_test))\n","\n","joblib.dump(pipeline, '../model/model_v1.joblib')"]},{"cell_type":"markdown","metadata":{"id":"O2reHQqd3miV"},"source":["__Moving on from train_test_split, let's try another validation method- one of the K-folds and, particularly, Stratified k-fold. This validation method is known to favour imbalanced dataset situations.\n","\n","Also, because this is an imbalanced data situation, I want to try out an approach where I create an ensemble of the models produced by each algorithm with stratified K-fold cross validation and use that to make predictions. Let's see what this looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrgRmJDT3mia"},"outputs":[],"source":["# import numpy as np\n","# new_skfold = StratifiedKFold(n_splits=10, shuffle= True, random_state=42)\n","\n","# for name, model in models.items():\n","#     pipeline = Pipeline([\n","#         ('scaler', scaler),\n","#         ('model', model)\n","#         ])\n","#     pipeline.fit(X_train, y_train)\n","#     scores = cross_val_score(pipeline, X_train, y_train, cv=new_skfold)\n","#     print(f\"The mean score for {name} is: {np.mean(scores)}, \\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcFRevcH3mia"},"outputs":[],"source":["# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n","# from sklearn.base import clone\n","# import numpy as np\n","\n","# # Define the number of folds and initialize variables\n","# n_splits = 10\n","# skfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","# models_per_algorithm = {}\n","\n","# # Initialize dictionaries to store metrics\n","# accuracy_scores = {}\n","# precision_scores = {}\n","# recall_scores = {}\n","# f1_scores = {}\n","# auprc_scores = {}\n","\n","# # Perform stratified k-fold cross-validation and store the models\n","# for name, model in models.items():\n","#     models_per_fold = []\n","#     for train_index, test_index in skfold.split(X_train, y_train):\n","#         X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n","#         y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n","\n","#         pipeline = Pipeline([\n","#             ('scaler', scaler),\n","#             ('model', model)\n","#         ])\n","\n","#         model = clone(pipeline)\n","#         model.fit(X_train_fold, y_train_fold)\n","#         models_per_fold.append(model)\n","\n","#     models_per_algorithm[name] = models_per_fold\n","\n","# # Combining models for each algorithm (one combined model per algorithm)\n","# combined_models = {}\n","# for algo_name, fold_models in models_per_algorithm.items():\n","#     combined_models[algo_name] = fold_models\n","\n","# # Making predictions using the combined models and calculating metrics\n","# ensemble_predictions = {}\n","# for algo_name, models_list in combined_models.items():\n","#     y_test_preds = [model.predict(X_test) for model in models_list]\n","#     y_test_pred = np.mean(y_test_preds, axis=0)\n","\n","#     # Calculate classification metrics\n","#     accuracy = accuracy_score(y_test, y_test_pred)\n","#     precision = precision_score(y_test, y_test_pred)\n","#     recall = recall_score(y_test, y_test_pred)\n","#     f1 = f1_score(y_test, y_test_pred)\n","#     auprc = average_precision_score(y_test, y_test_pred)\n","\n","#     # Store the metrics in dictionaries\n","#     accuracy_scores[algo_name] = accuracy\n","#     precision_scores[algo_name] = precision\n","#     recall_scores[algo_name] = recall\n","#     f1_scores[algo_name] = f1\n","#     auprc_scores[algo_name] = auprc\n","\n","#     print(f\"Metrics for {algo_name}:\")\n","#     print(\"Accuracy:\", accuracy)\n","#     print(\"Precision:\", precision)\n","#     print(\"Recall:\", recall)\n","#     print(\"F1 Score:\", f1)\n","#     print(\"AUPRC:\", auprc)\n","#     print(\"------------------------\")\n","\n","# # Print or return the metrics dictionaries if needed\n","# print(\"Accuracy Scores:\", accuracy_scores)\n","# print(\"Precision Scores:\", precision_scores)\n","# print(\"Recall Scores:\", recall_scores)\n","# print(\"F1 Scores:\", f1_scores)\n","# print(\"AUPRC Scores:\", auprc_scores)"]},{"cell_type":"markdown","metadata":{"id":"yOmm9ge53mia"},"source":["We just need to be careful to leverage training tools that are more favourable towards imbalanced data. For e.g, stratified k-fold, for cross validation instead of just k-fold."]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}